# Artifact Submission Template

Please answer the following questions concisely, either with bullet lists or
short paragraphs.

Title of the submitted paper:
_Do Machine Learning Models Produce TypeScript Types that Type Check?_

ECOOP submission number for the paper: 71

## Overview: What does the artifact comprise?

Please list for each distinct component of your artifact:

* what type of artifact (data, code, proof, etc.)
    - Data (benchmarks and results) and code

* in which format (e.g., CSV, source code, binary, etc.)
    - Benchmarks are source code (NPM packages)
    - Results are log files, source code (TypeScript), and CSV files
    - Code is Python, TypeScript, and R

* can be found in which location
    - Benchmarks: `data/full/`
    - Results: `data/results/`
        - Raw results: `data/results/*-out/`
        - CSV files summarizing the raw results: `data/results/csv/`
        - Figures and tables produced from the CSVs: `data/results/figures/`
    - Source code:
        - `src/`

* Which badges do you claim for your artifact?
    - Functional + reusable
    - Available

## For authors claiming a functional or reusable badge: What are claims about the artifact’s functionality to be evaluated by the committee?

Please provide explicit references that link processes, data, or conclusions in
the paper with the location of the supporting component in the artifact.

- We use the following mapping for datasets in the paper:
    - `top1k-typed-nodeps` → "DefinitelyTyped, no deps"
    - `top1k-typed-with-typed-deps` → "DefinitelyTyped, with deps"
    - `top1k-untyped-nodeps` → "Never typed, no deps"
    - `top1k-untyped-with-typed-deps` → "Never typed, with deps"
    - If the dataset has the `-es6` suffix, it was the result of applying the
      step in §3.1.

- Note: Figures 1-3 and 12-15 are examples and not generated from the data

- All other figures and tables can be generated from the CSVs
    - Running `make figures` will:
        - copy the CSVs from `data/results/csv/` to `data/full/csv/`
        - run the R script `src/R/figures.R`, which reads the files in
          `data/full/csv/` and outputs to `data/full/figures/`
    - The pre-generated tables and figures used in the paper are in
      `data/results/figures/`
    - For the rest of this section, look in `data/full/figures/` for the
      produced tables (`*.tex`) and figures (`*.pdf`)

- Abstract
    - The number of packages (513), percent of packages that type check (21%),
      and percent of files that type check (69%) are saved in `constants.tex`
      and generated by `figures.R` on lines 71, 153, and 236

- §4.1 Dataset
    - Performing the dataset construction described in §4.1 results in:
        - datasets saved in `data/full/original/`
        - dependency type declarations listed in `data/full/package.json`
        - dependency type declarations saved in `data/full/node_modules`
        - ground truth type declarations saved in `data/full/groundtruth`
    - Table 1: `dataset_summary.tex`, `figures.R:82`
    - Figure 4: `package_loc_cdf.pdf`, `figure.R:94`

- §4.2 Success Rate of Type Checking
    - Do Migrated Packages Type Check?
        - Table 2: `pkg_typechecks.tex`, `figures.R:137`
        - Figure 5: `pkg_typechecks.pdf`, `figures.R:175`
    - How Many Files are Error Free?
        - Table 3: `files_no_errors.tex`, `figures.R:220`
        - Figure 6: `files_no_errors.pdf`, `figures.R:258`
        - Figure 7: `errorfree_per_package.pdf`, `figures.R:298`
    - Do Migrated Types Match Human-Written Types (When Available)?
        - Figure 8: `accuracy.pdf`, `figures.R:381`
        - Table 4: `accuracy.tex`, `figures.R:359`
    - How Many Errors Occur in Each Package?
        - Figure 9: `error_cdf.pdf`, `figures.R:413`

- §4.3 Error Analysis
    - Table 5: `error_codes.tex`, `figures.R:492`
    - Figure 10: `error_codes.pdf`, `figures.R:515`

- §4.4 ECMAScript Module Conversion
    - Table 6: `typecheck_comparison.tex`, `figures.R:566`
    - Table 7: `files_no_errors_comparison.tex`, `figures.R:615`
    - Table 8: `accuracy_comparison.tex`, `figures.R:684`
    - Figure 11: `typechecks_es6.pdf`, `figures.R:722`

## For authors claiming a reusable badge: What are the authors' claims about the artifact's reusability to be evaluated by the committee?

Please list any reuse scenarios that you envision for your artifact, i.e.,
state how the artifact can be reused or repurposed beyond its role in the
submitted paper.

TODO
- new dataset
- new model
- playground


* “The implementation can easily be modified to use a different algorithm than the one described in section 4.1 of our paper by implementing a corresponding module. We provide an interface specification in ...”

## For authors claiming an available badge

We offer to publish the artifact on
[DARTS](https://drops.dagstuhl.de/opus/institut_darts.php), in which case the
available badge will be issued automatically. If you agree to publishing your
artifact under a Creative Commons license, please indicate this here.

- Yes

## Artifact Requirements

Please list any specific hardware or software requirements for accessing your
artifact.

- We are providing the reviewers with SSH access to a VM with the hardware and
  software already set up:
    - `ssh reviewer@[REDACTED]`
    - Password: `[REDACTED]`
    - The home directory is `/reviewerhome` and contains a copy of the artifact
      tarball
    - **Important**: Reviewers, please coordinate with each other to share
      access to the VM, to avoid clobbering files or consuming all resources
    - You can use the `reviewer1`, `reviewer2`, and `reviewer2` subdirectories
    - Nobody else will be using the VM
    - The VM has a 12-core Intel Xeon processor @ 2.9 GHz, 94 GB of RAM,
      and an NVIDIA A100 with 40 GB of VRAM

- To run on your own machine, you will need:
    - Hardware: a GPU with at least 14 GB of VRAM
        - It is possible to skip the experiments that require a GPU; see the
          README for details.
    - Software:
        - Linux
        - Python +3.6 and the `tqdm` package (`pip install tqdm`)
        - [Podman](https://podman.io/) with the [NVIDIA container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#podman)
            - All other dependencies are managed with OCI images
            - Docker may be used instead of Podman, but has not been tested

## Getting Started

Please briefly describe how to get started with your artifact.

- If you are using the VM:
    1. `ssh reviewer@[REDACTED]` with password `[REDACTED]`, and extract
       `~/TypeWeaver-artifact.tar.gz`
    2. Run `make build` to build the containers. The containers on the VM are
       pre-built, so it should take a few minutes.
    3. Run `make micro` to run the evaluation pipeline on a single project.
       This should take 5 minutes, and checks that all the containers are
       working.
        - Inspect the DeepTyper and LambdaNet results by running
        `less data/micro/*-out/top1k-typed-nodeps-es6/baseline/decamelize/index.ts`.
        InCoder is not expected to produce a result. Compare to the original
        JavaScript source by running
        `less data/micro/original/top1k-typed-nodeps-es6/decamelize/index.js`
    4. Recreate the tables and figures by running `make figures`. This will create
       `data/full/figures/` and take a few seconds.
        - Compare the generated figures (`data/full/figures/`) with the figures
        used in the paper (`results/figures/`), by running
        `diff data/full/figures/ data/results/figures/`. The tables should match
        exactly. The PDFs should match visually, but are not binary matches.

- If you are using your own machine:
    1. Download and extract the tarball.
    2. Run `make build` to build the containers. This may take up to an hour and
       use up to 30 GB of space.
    3. Run `make micro` (or `make NOGPU=true micro` if you do not have a GPU).
       This should take 5 minutes, and checks that all the containers are
       working.
        - Inspect the DeepTyper and LambdaNet results by running
        `less data/micro/*-out/top1k-typed-nodeps-es6/baseline/decamelize/index.ts`.
        InCoder is not expected to produce a result. Compare to the original
        JavaScript source by running
        `less data/micro/original/top1k-typed-nodeps-es6/decamelize/index.js`
    4. Recreate the tables and figures by running `make figures`. This will create
       `data/full/figures/` and take a few seconds.
        - Compare the generated figures (`data/full/figures/`) with the figures
        used in the paper (`results/figures/`), by running
        `diff data/full/figures/ data/results/figures/`. The tables should match
        exactly. The PDFs should match visually, but are not binary matches.

We estimate that running the full experiments on the provided VM will take over
30 hours. Please see the README for instructions on running the experiments, as
well as how to skip stages of the experiment.
